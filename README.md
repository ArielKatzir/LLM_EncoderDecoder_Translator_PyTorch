# LLM_EncoderDecoder_Translator_PyTorch
This repository includes an implementation of an encoder-decoder transformer architecture using PyTorch. I use a similar architecture to the one in the paper "Attention Is All You Need", which uses an attention mechanism to connect the encoder and the decoder. For educational purposes, I first implemented it in a light manner, where I tried to miss out on some useful techniques to see how the model will perform at a basic level. This achieved a BLEU score of around 15. With the same amount of small training data, I then improved the model in a new notebook while using Claude for guidance and recommendations for improvements. This achieved a BLEU score of around 30, doubling my first attempt. 
This was designed using colab on a training sample of 30000 for training, and 5000 for each test and val. 
