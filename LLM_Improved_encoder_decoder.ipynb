{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1kDn4YZtlEYLcZ6f0-egtbKFotLUERjht",
      "authorship_tag": "ABX9TyP1ReRKZTqllEvl0cc1IvYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArielKatzir/LLM_EncoderDecoder_Translator_PyTorch/blob/master/LLM_Improved_encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small Scale Encoder Decoder LLM Translating English to Italian\n",
        "### Improvements to the basic code\n"
      ],
      "metadata": {
        "id": "PsTooFVQo1H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "EDip-r0vOsSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8f0ab5-bd61-4e72-8bce-db7aa1733355"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import sacrebleu\n",
        "import torch\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import defaultdict\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# data downloaded from https://tatoeba.org/en/downloads\n",
        "raw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/english_italian.tsv', sep='\\t', on_bad_lines='skip')\n",
        "raw_df.shape, raw_df.head(1)"
      ],
      "metadata": {
        "id": "ChEGayLkaBjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd024b9-6831-49e2-dfc9-9fadb50afc50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((681865, 4),\n",
              "    1276    Let's try something.  565618      Proviamo qualcosa!\n",
              " 0  1277  I have to go to sleep.  4369.0  Devo andare a dormire.)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_english = np.array(raw_df[raw_df.columns[1]])\n",
        "t_italian = np.array(raw_df[raw_df.columns[3]])\n",
        "\n",
        "mask = np.random.rand(t_english.shape[0]) > 0.1\n",
        "\n",
        "masked_indices = np.where(mask)[0]\n",
        "assert len(t_english) == len(t_italian), \"Mismatched lengths!\"\n",
        "\n",
        "t_eng_sample = t_english[masked_indices][:500000]\n",
        "t_ita_sample = t_italian[masked_indices][:500000]\n",
        "\n",
        "t_eng_sample.shape, t_ita_sample.shape"
      ],
      "metadata": {
        "id": "u-zDs-M4dD88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31aad0b8-389e-4e49-df52-5c533d932203"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500000,), (500000,))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"english_italian_tokanizer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for en, it in zip(t_eng_sample, t_ita_sample):\n",
        "        f.write(en.strip() + \"\\n\")\n",
        "        f.write(it.strip() + \"\\n\\n\")"
      ],
      "metadata": {
        "id": "ST0NfTVrmcqq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    input='english_italian_tokanizer.txt',\n",
        "    model_prefix='bpe',\n",
        "    vocab_size=15000,\n",
        "    character_coverage=1.0,\n",
        "    model_type='bpe',\n",
        "    pad_id=0, pad_piece='<pad>',\n",
        "    unk_id=1, unk_piece='<unk>',\n",
        "    bos_id=2, bos_piece='<s>',\n",
        "    eos_id=3, eos_piece='</s>'\n",
        ")"
      ],
      "metadata": {
        "id": "fkJDeX_ho2TV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"bpe.model\")"
      ],
      "metadata": {
        "id": "uBJITMkrqP4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fba0248-0536-4e00-a88e-4d6201115efd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets take our training data sample. Make it small coz we dont have much GPU power."
      ],
      "metadata": {
        "id": "takLrOJGygRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 50\n",
        "sos_id = tokenizer.piece_to_id('<s>')\n",
        "eos_id = tokenizer.piece_to_id('</s>')\n",
        "pad_id = tokenizer.piece_to_id('<pad>')\n",
        "unk_id = tokenizer.unk_id()"
      ],
      "metadata": {
        "id": "5j8P2tQkyrNb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved training configurations per use case\n",
        "\n",
        "# original setup - 30000 samples, BLEU=30\n",
        "def get_improved_config():\n",
        "    return {\n",
        "        'vocab_size': 15000,\n",
        "        'd_model': 256,\n",
        "        'n_heads': 8,\n",
        "        'max_len': 64,\n",
        "        'num_blocks': 4,\n",
        "        'batch_size': 128,\n",
        "        'epochs': 20,\n",
        "        'dropout': 0.3,\n",
        "        'lr': 5e-4,\n",
        "        'weight_decay': 0.01,\n",
        "        'label_smoothing': 0.1,\n",
        "        'warmup_steps': 4000,\n",
        "        'patience': 5,  # Early stopping\n",
        "        'grad_clip': 1.0,\n",
        "        'eval_every': 500,  # Evaluate every N steps\n",
        "    }\n",
        "# Calude suggestions\n",
        "def get_config_for_large_dataset():\n",
        "    \"\"\"\n",
        "    Optimized configuration for 700K training samples\n",
        "    Balances performance with training time\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Model architecture - slightly larger for more data\n",
        "        'vocab_size': 15000,\n",
        "        'd_model': 320,          # Increased from 256 (more capacity for larger dataset)\n",
        "        'n_heads': 8,            # Keep same\n",
        "        'max_len': 64,           # Keep same\n",
        "        'num_blocks': 6,         # Increased from 4 (can handle more complexity)\n",
        "\n",
        "        # Training efficiency\n",
        "        'batch_size': 256,       # Increased from 128 (better GPU utilization)\n",
        "        'epochs': 8,             # Reduced from 20 (fewer epochs needed with more data)\n",
        "\n",
        "        # Regularization - reduced since we have more data\n",
        "        'dropout': 0.2,          # Reduced from 0.3 (less overfitting risk)\n",
        "        'weight_decay': 0.005,   # Reduced from 0.01\n",
        "        'label_smoothing': 0.1,  # Keep same\n",
        "\n",
        "        # Learning rate - adjusted for larger batches and dataset\n",
        "        'lr': 8e-4,              # Increased from 5e-4 (larger batches need higher LR)\n",
        "        'warmup_steps': 6000,    # Increased from 4000 (longer warmup for stability)\n",
        "\n",
        "        # Training control\n",
        "        'patience': 3,           # Reduced from 5 (converges faster with more data)\n",
        "        'grad_clip': 1.0,        # Keep same\n",
        "        'eval_every': 1000,      # Increased from 500 (less frequent eval saves time)\n",
        "\n",
        "        # Data efficiency\n",
        "        'accumulation_steps': 1, # No gradient accumulation needed with larger batches\n",
        "        'num_workers': 4,        # More data loading workers\n",
        "    }\n",
        "\n",
        "def get_config_for_maximum_performance():\n",
        "    \"\"\"\n",
        "    Configuration for maximum BLEU score (if GPU time is less concern)\n",
        "    Expected BLEU: 45-55\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'vocab_size': 15000,\n",
        "        'd_model': 384,          # Even larger model\n",
        "        'n_heads': 8,\n",
        "        'max_len': 64,\n",
        "        'num_blocks': 8,         # Deeper model\n",
        "\n",
        "        'batch_size': 192,       # Slightly smaller due to larger model\n",
        "        'epochs': 12,            # A few more epochs for maximum performance\n",
        "\n",
        "        'dropout': 0.15,         # Lower dropout with more data\n",
        "        'weight_decay': 0.003,\n",
        "        'label_smoothing': 0.1,\n",
        "\n",
        "        'lr': 6e-4,\n",
        "        'warmup_steps': 8000,    # Longer warmup for deeper model\n",
        "\n",
        "        'patience': 4,\n",
        "        'grad_clip': 1.0,\n",
        "        'eval_every': 1500,\n",
        "        'num_workers': 4,\n",
        "    }\n",
        "# I will use this one\n",
        "def get_config_for_fast_training():\n",
        "    \"\"\"\n",
        "    Configuration for quick experimentation\n",
        "    Expected BLEU: 35-42 (good but not maximum)\n",
        "    Much faster training time\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'vocab_size': 15000,\n",
        "        'd_model': 256,          # Keep original size\n",
        "        'n_heads': 8,\n",
        "        'max_len': 64,\n",
        "        'num_blocks': 4,         # Keep shallow\n",
        "\n",
        "        'batch_size': 384,       # Large batches for speed\n",
        "        'epochs': 5,             # Very few epochs\n",
        "\n",
        "        'dropout': 0.1,          # Low dropout\n",
        "        'weight_decay': 0.002,\n",
        "        'label_smoothing': 0.1,\n",
        "\n",
        "        'lr': 1e-3,              # Higher learning rate\n",
        "        'warmup_steps': 3000,    # Shorter warmup\n",
        "\n",
        "        'patience': 2,           # Early stop quickly\n",
        "        'grad_clip': 1.0,\n",
        "        'eval_every': 2000,      # Less frequent evaluation\n",
        "        'num_workers': 4,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# but do we even have a GPU???\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "QXBDxYw4MrGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84c4f89-3dca-4833-9732-d30a7f8a0456"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We improve this class by adding more filters for input sentances.\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len):\n",
        "        self.src = src_sentences\n",
        "        self.tgt = tgt_sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Filter out sentences that are too long or too short\n",
        "        self.filtered_pairs = []\n",
        "        for s, t in zip(src_sentences, tgt_sentences):\n",
        "            src_ids = self.tokenizer.encode(s, out_type=int)\n",
        "            tgt_ids = self.tokenizer.encode(t, out_type=int)\n",
        "\n",
        "            # Skip very short or very long sentences\n",
        "            if 2 <= len(src_ids) <= max_len-2 and 2 <= len(tgt_ids) <= max_len-3:\n",
        "                self.filtered_pairs.append((s, t))\n",
        "\n",
        "        print(f\"Filtered dataset: {len(self.filtered_pairs)} pairs from {len(src_sentences)} original\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text, tgt_text = self.filtered_pairs[idx]\n",
        "\n",
        "        src_ids = self.tokenizer.encode(src_text, out_type=int)\n",
        "        tgt_ids = self.tokenizer.encode(tgt_text, out_type=int)\n",
        "\n",
        "        # Add BOS/EOS to target\n",
        "        sos_id = self.tokenizer.piece_to_id('<s>')\n",
        "        eos_id = self.tokenizer.piece_to_id('</s>')\n",
        "        pad_id = self.tokenizer.piece_to_id('<pad>')\n",
        "\n",
        "        # Ensure we don't exceed max length\n",
        "        src_ids = src_ids[:self.max_len-2]  # Leave room for potential special tokens\n",
        "        tgt_ids = [sos_id] + tgt_ids[:self.max_len-2] + [eos_id]\n",
        "\n",
        "        # Padding\n",
        "        src_ids += [pad_id] * (self.max_len - len(src_ids))\n",
        "        tgt_ids += [pad_id] * (self.max_len - len(tgt_ids))\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_ids, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1SMofQhGNMP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, max_len):\n",
        "    super().__init__()\n",
        "    self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "  def forward(self, x):\n",
        "    positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "    return self.token_embed(x) + self.pos_embed(positions)\n",
        "\n",
        "# Actually initialise the embeddings and add dropout to them. We also root the embeddings for...\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize embeddings properly\n",
        "        nn.init.normal_(self.token_embed.weight, 0, 0.02)\n",
        "        nn.init.normal_(self.pos_embed.weight, 0, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
        "        # Scale embeddings by sqrt(d_model) as in original Transformer\n",
        "        embeddings = self.token_embed(x) * math.sqrt(self.d_model) + self.pos_embed(positions)\n",
        "        return self.dropout(embeddings)"
      ],
      "metadata": {
        "id": "0FgIdO1ILkNV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, heads):\n",
        "    super().__init__()\n",
        "\n",
        "    # creates Q,K,V projected inputs and returns the attention matrix\n",
        "    self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    # fast forward\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(d_model, 4*d_model),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(4*d_model, d_model),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input is of dim: (B,T,d_model)\n",
        "    B, T, _ = x.shape\n",
        "\n",
        "    # No masking is needed here because its the encoder\n",
        "\n",
        "    attn_output, _ = self.attn(x, x, x)\n",
        "    x = self.ln1(x + attn_output)\n",
        "    x = self.ln2(x + self.ffn(x))\n",
        "    return x\n",
        "\n",
        "# add dropout to the attention layer. switch back to ReLU\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads,\n",
        "                                         dropout=dropout, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, 4*d_model),\n",
        "            nn.ReLU(),  # ReLU is more stable than GELU for training\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4*d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # Self-attention with proper masking\n",
        "        attn_output, _ = self.attn(x, x, x, key_padding_mask=src_key_padding_mask)\n",
        "        x = self.ln1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.ln2(x + ffn_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uW2shvOLOfom"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # also use dropout here\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # ReLU\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # Self-attention on target\n",
        "        self_attn_output, _ = self.self_attn(x, x, x, attn_mask=tgt_mask,\n",
        "                                           key_padding_mask=tgt_key_padding_mask)\n",
        "        x = self.ln1(x + self_attn_output)\n",
        "\n",
        "        # Cross-attention with encoder output\n",
        "        cross_attn_output, _ = self.cross_attn(x, memory, memory,\n",
        "                                             key_padding_mask=memory_key_padding_mask)\n",
        "        x = self.ln2(x + cross_attn_output)\n",
        "\n",
        "        # FFN\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.ln3(x + ffn_output)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "7KY_EmrdVe3I"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, max_len, num_blocks, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared embeddings can help with small datasets\n",
        "        self.embedding = Embeddings(vocab_size, d_model, max_len, dropout)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_heads, dropout) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, dropout) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # Embeddings - combined\n",
        "        src_embedded = self.embedding(src)\n",
        "        tgt_embedded = self.embedding(tgt)\n",
        "\n",
        "        # encoder - with padding mask\n",
        "        memory = src_embedded\n",
        "        for block in self.encoder_blocks:\n",
        "            memory = block(memory, src_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        # decoder\n",
        "        x = tgt_embedded\n",
        "        for block in self.decoder_blocks:\n",
        "            x = block(x, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "HUISbj2Gk0bO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_causal_mask(size: int, device: torch.device = None):\n",
        "    mask = torch.triu(torch.full((size, size), float('-inf')), diagonal=1)\n",
        "    if device is not None:\n",
        "        mask = mask.to(device)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "aCjOGzwTJGlJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, tokenizer, loss_fn, pad_id, eos_id, generate_causal_mask, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_tokens = 0\n",
        "    total_tokens = 0\n",
        "    all_preds = []\n",
        "    all_refs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
        "\n",
        "            # Create masks\n",
        "            tgt_mask = generate_causal_mask(tgt_input.size(1), device)\n",
        "            tgt_kp = (tgt_input == pad_id)\n",
        "            mem_kp = (src == pad_id)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(src, tgt_input,\n",
        "                         tgt_mask=tgt_mask,\n",
        "                         tgt_key_padding_mask=tgt_kp,\n",
        "                         memory_key_padding_mask=mem_kp)\n",
        "\n",
        "            B, T, V = logits.size()\n",
        "            loss = loss_fn(logits.view(B * T, V), tgt_output.reshape(B * T))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Token accuracy\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            mask = (tgt_output != pad_id)\n",
        "            correct_tokens += ((preds == tgt_output) & mask).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "            # BLEU calculation - properly handle EOS tokens\n",
        "            for i in range(B):\n",
        "                # Reference: find EOS and cut there\n",
        "                ref_list = tgt_output[i].tolist()\n",
        "                if eos_id in ref_list:\n",
        "                    eos_pos = ref_list.index(eos_id)\n",
        "                    ref_ids = [tid for tid in ref_list[:eos_pos] if tid != pad_id]\n",
        "                else:\n",
        "                    ref_ids = [tid for tid in ref_list if tid not in {pad_id}]\n",
        "\n",
        "                # Prediction: do the same\n",
        "                pred_list = preds[i].tolist()\n",
        "                if eos_id in pred_list:\n",
        "                    eos_pos = pred_list.index(eos_id)\n",
        "                    pred_ids = [tid for tid in pred_list[:eos_pos] if tid != pad_id]\n",
        "                else:\n",
        "                    pred_ids = [tid for tid in pred_list if tid not in {pad_id}]\n",
        "\n",
        "                ref_text = tokenizer.decode(ref_ids, out_type=str).strip()\n",
        "                pred_text = tokenizer.decode(pred_ids, out_type=str).strip()\n",
        "\n",
        "                all_refs.append(ref_text)\n",
        "                all_preds.append(pred_text)\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    ppl = math.exp(min(avg_loss, 10))  # Cap perplexity to avoid overflow\n",
        "    accuracy = correct_tokens / max(total_tokens, 1) * 100.0\n",
        "\n",
        "    # Calculate BLEU and chrF\n",
        "    try:\n",
        "        bleu = sacrebleu.corpus_bleu(all_preds, [all_refs]).score\n",
        "        chrf = sacrebleu.corpus_chrf(all_preds, [all_refs]).score\n",
        "    except:\n",
        "        bleu, chrf = 0.0, 0.0\n",
        "\n",
        "    return {\n",
        "        'avg_loss': avg_loss,\n",
        "        'ppl': ppl,\n",
        "        'accuracy': accuracy,\n",
        "        'bleu': bleu,\n",
        "        'chrf': chrf,\n",
        "        'sample_preds': all_preds[:5],\n",
        "        'sample_refs': all_refs[:5]\n",
        "    }\n",
        "\n",
        "def plot_training_curves(stats):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training loss\n",
        "    ax1.plot(stats['train_losses'])\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Validation loss\n",
        "    ax2.plot(stats['val_losses'])\n",
        "    ax2.set_title('Validation Loss')\n",
        "    ax2.set_xlabel('Evaluation Step')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # BLEU scores\n",
        "    ax3.plot(stats['val_bleus'])\n",
        "    ax3.set_title('Validation BLEU Score')\n",
        "    ax3.set_xlabel('Evaluation Step')\n",
        "    ax3.set_ylabel('BLEU')\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Learning rate\n",
        "    ax4.plot(stats['learning_rates'])\n",
        "    ax4.set_title('Learning Rate Schedule')\n",
        "    ax4.set_xlabel('Step')\n",
        "    ax4.set_ylabel('Learning Rate')\n",
        "    ax4.set_yscale('log')\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DdHk4_qnOVnX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved learning rate scheduler\n",
        "def get_transformer_scheduler(optimizer, d_model, warmup_steps):\n",
        "    \"\"\"Transformer learning rate schedule with warmup\"\"\"\n",
        "    def lr_lambda(step):\n",
        "        if step == 0:\n",
        "            step = 1\n",
        "        return min(step ** -0.5, step * warmup_steps ** -1.5) * (d_model ** -0.5)\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sXTcyb3yMfWg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same amount of data as the basic code\n",
        "all_english = np.array([str(s) for s in t_english.tolist()])\n",
        "all_italian = np.array([str(s) for s in t_italian.tolist()])\n",
        "\n",
        "mask = np.random.rand(len(all_english)) > 0.5\n",
        "candidate_idxs = np.where(mask)[0]\n",
        "train_idxs     = candidate_idxs[:400000]\n",
        "\n",
        "train_en = all_english[train_idxs]\n",
        "train_it = all_italian[train_idxs]\n",
        "\n",
        "remaining_idxs = np.setdiff1d(np.arange(len(all_english)), train_idxs)\n",
        "\n",
        "val_idxs, test_idxs = train_test_split(\n",
        "    remaining_idxs,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "max_test_val = 50000\n",
        "\n",
        "val_en  = all_english[val_idxs][:max_test_val]\n",
        "val_it  = all_italian[val_idxs][:max_test_val]\n",
        "test_en = all_english[test_idxs][:max_test_val]\n",
        "test_it = all_italian[test_idxs][:max_test_val]\n",
        "\n",
        "train_en.shape, train_it.shape, val_en.shape, val_en.shape, test_en.shape, test_en.shape"
      ],
      "metadata": {
        "id": "-O5V8_LfNbdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98bb8c2-9877-410a-9c1c-610ba481947d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((340762,), (340762,), (50000,), (50000,), (50000,), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Real Shit - Optimization\n",
        "\n",
        "So our model is small scale, yes, but it should be able to do better without increasing things like the size of the training data or training time. I want to try two methods for making this better.\n",
        "\n",
        "---> Validation - based training - which i should have done from the start but i wanted to see how it does without it. No bueno.\n",
        "\n",
        "---> Attention visualization - so you see what your model is focusing on and can diagnose weaknesses.\n",
        "\n"
      ],
      "metadata": {
        "id": "XavYjP_vzhwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets take a look first at our training loop and try to add the following.\n",
        "\n",
        "Key Features of improvements:\n",
        "label smoothing - Instead of giving correct labels a 1 and the rest a 0. Smooth it out with a 0.9 or something.\n",
        "\n",
        "Progressive Evaluation: Evaluates every 500 steps instead of just at epoch end\n",
        "\n",
        "Early Stopping: Stops if validation loss doesn't improve for 5 evaluations\n",
        "\n",
        "Mixed Precision: Uses automatic mixed precision for faster training\n",
        "\n",
        "Gradient Clipping: Prevents gradient explosion\n",
        "\n",
        "Proper Learning Rate: Transformer schedule with warmup\n",
        "\n",
        "Sample Translations: Shows actual translations during training\n",
        "\n",
        "Comprehensive Logging: Tracks all important metrics\n",
        "\n",
        "Model Checkpointing: Saves best model automatically\n",
        "\n",
        "Training Visualization: Plots training curves\n",
        "\n",
        "Testing Function: Easy way to test your trained model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xd0qt35k7hV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer IDs\n",
        "pad_id = tokenizer.piece_to_id('<pad>')\n",
        "eos_id = tokenizer.piece_to_id('</s>')\n",
        "\n",
        "# modified calude config\n",
        "config = get_config_for_maximum_performance()\n",
        "\n",
        "# Create datasets with improved filtering\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = TranslationDataset(train_en, train_it, tokenizer, config['max_len'])\n",
        "val_dataset = TranslationDataset(val_en, val_it, tokenizer, config['max_len'])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,  # Speed up data loading\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "4Ia5X0D1UuiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8849d3-df6b-4410-c8f7-c4443dc2fa59"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets...\n",
            "Filtered dataset: 340546 pairs from 340762 original\n",
            "Filtered dataset: 49979 pairs from 50000 original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "print(\"Initializing model...\")\n",
        "model = EncoderDecoderModel(\n",
        "    vocab_size=config['vocab_size'],\n",
        "    d_model=config['d_model'],\n",
        "    n_heads=config['n_heads'],\n",
        "    max_len=config['max_len'],\n",
        "    num_blocks=config['num_blocks'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "f\"Total parameters: {total_params:,}\" , f\"Trainable parameters: {trainable_params:,}\""
      ],
      "metadata": {
        "id": "74g0j5-ZVx_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9772448e-c8c4-43f2-d1ea-575cfe53862e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Total parameters: 44,688,024', 'Trainable parameters: 44,688,024')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function with label smoothing\n",
        "loss_fn = nn.CrossEntropyLoss(\n",
        "    ignore_index=pad_id,\n",
        "    label_smoothing=config['label_smoothing']\n",
        ")\n",
        "\n",
        "# Optimizer with transformer-specific settings\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=config['weight_decay'],\n",
        "    betas=(0.9, 0.98),  # Transformer-specific betas\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "def lr_lambda(step):\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    # Transformer learning rate schedule\n",
        "    warmup_factor = min(step ** -0.5, step * config['warmup_steps'] ** -1.5)\n",
        "    return warmup_factor * (config['d_model'] ** -0.5) * (config['lr'] ** -1)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training tracking\n",
        "training_stats = {\n",
        "    'train_losses': [],\n",
        "    'val_losses': [],\n",
        "    'val_bleus': [],\n",
        "    'val_accuracies': [],\n",
        "    'learning_rates': [],\n",
        "    'step_times': []\n",
        "}\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "best_bleu = 0.0\n",
        "patience_counter = 0\n",
        "global_step = 0"
      ],
      "metadata": {
        "id": "B2Sbmh8LV56e"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, config['epochs'] + 1):\n",
        "    # training\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        step_start_time = time.time()\n",
        "\n",
        "        # to device\n",
        "        src = batch['src'].to(device, non_blocking=True)\n",
        "        tgt = batch['tgt'].to(device, non_blocking=True)\n",
        "\n",
        "        # decoder input/output\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        # masks\n",
        "        tgt_mask = generate_causal_mask(tgt_input.size(1), device)\n",
        "        tgt_key_padding_mask = (tgt_input == pad_id)\n",
        "        memory_key_padding_mask = (src == pad_id)\n",
        "\n",
        "        # reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with autocast('cuda'):\n",
        "            logits = model(\n",
        "                src,\n",
        "                tgt_input,\n",
        "                tgt_mask=tgt_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            # Compute loss\n",
        "            B, T, V = logits.size()\n",
        "            loss = loss_fn(\n",
        "                logits.reshape(B * T, V),\n",
        "                tgt_output.reshape(B * T)\n",
        "            )\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
        "\n",
        "        # Optimizer step\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update statistics\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_steps += 1\n",
        "        global_step += 1\n",
        "\n",
        "        step_time = time.time() - step_start_time\n",
        "        training_stats['step_times'].append(step_time)\n",
        "        training_stats['learning_rates'].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, \"\n",
        "                  f\"Loss: {loss.item():.4f}, LR: {current_lr:.2e}, \"\n",
        "                  f\"Step time: {step_time:.3f}s\")\n",
        "\n",
        "        # Evaluate periodically\n",
        "        if global_step % config['eval_every'] == 0:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"EVALUATION AT STEP {global_step}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            val_metrics = evaluate(\n",
        "                model, val_loader, tokenizer, loss_fn,\n",
        "                pad_id, eos_id, generate_causal_mask, device\n",
        "            )\n",
        "\n",
        "            # Log metrics\n",
        "            training_stats['val_losses'].append(val_metrics['avg_loss'])\n",
        "            training_stats['val_bleus'].append(val_metrics['bleu'])\n",
        "            training_stats['val_accuracies'].append(val_metrics['accuracy'])\n",
        "\n",
        "            print(f\"Validation Loss: {val_metrics['avg_loss']:.4f}\")\n",
        "            print(f\"Validation Perplexity: {val_metrics['ppl']:.2f}\")\n",
        "            print(f\"Token Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
        "            print(f\"BLEU Score: {val_metrics['bleu']:.2f}\")\n",
        "            print(f\"chrF Score: {val_metrics['chrf']:.2f}\")\n",
        "\n",
        "            # Show sample translations\n",
        "            print(\"\\nSample Translations:\")\n",
        "            print(\"-\" * 40)\n",
        "            for i, (pred, ref) in enumerate(zip(val_metrics['sample_preds'][:3],\n",
        "                                              val_metrics['sample_refs'][:3])):\n",
        "                print(f\"Reference {i+1}: {ref}\")\n",
        "                print(f\"Prediction {i+1}: {pred}\")\n",
        "                print()\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_metrics['avg_loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['avg_loss']\n",
        "                best_bleu = val_metrics['bleu']\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save best model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'global_step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'val_loss': val_metrics['avg_loss'],\n",
        "                    'bleu': val_metrics['bleu'],\n",
        "                    'config': config\n",
        "                }, 'best_model.pt')\n",
        "\n",
        "                print(f\"âœ“ New best model saved! BLEU: {best_bleu:.2f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"No improvement. Patience: {patience_counter}/{config['patience']}\")\n",
        "\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= config['patience']:\n",
        "                print(f\"Early stopping triggered after {global_step} steps\")\n",
        "                break\n",
        "\n",
        "    # End of epoch\n",
        "    avg_train_loss = epoch_loss / epoch_steps\n",
        "    training_stats['train_losses'].append(avg_train_loss)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch} completed. Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Break if early stopping triggered\n",
        "    if patience_counter >= config['patience']:\n",
        "        break\n",
        "\n",
        "# =========================\n",
        "# TRAINING COMPLETE\n",
        "# =========================\n",
        "total_time = time.time() - start_time\n",
        "avg_step_time = sum(training_stats['step_times']) / len(training_stats['step_times'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total training time: {total_time/3600:.2f} hours\")\n",
        "print(f\"Average step time: {avg_step_time:.3f} seconds\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best BLEU score: {best_bleu:.2f}\")\n",
        "print(f\"Total steps: {global_step}\")\n",
        "\n",
        "# Load best model for final evaluation\n",
        "print(\"\\nLoading best model for final evaluation...\")\n",
        "checkpoint = torch.load('best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Final evaluation on validation set\n",
        "print(\"Final validation evaluation:\")\n",
        "final_val_metrics = evaluate(\n",
        "    model, val_loader, tokenizer, loss_fn,\n",
        "    pad_id, eos_id, generate_causal_mask, device\n",
        ")\n",
        "\n",
        "print(f\"Final Validation Loss: {final_val_metrics['avg_loss']:.4f}\")\n",
        "print(f\"Final BLEU Score: {final_val_metrics['bleu']:.2f}\")\n",
        "print(f\"Final Token Accuracy: {final_val_metrics['accuracy']:.2f}%\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_curves(training_stats)"
      ],
      "metadata": {
        "id": "HAeOOShAWbd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7cbe0e-8971-4d42-e08b-a1b0f0256c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 0/1773, Loss: 9.6490, LR: 7.13e-08, Step time: 1.709s\n",
            "Epoch 1, Batch 100/1773, Loss: 9.0265, LR: 7.20e-06, Step time: 0.243s\n",
            "Epoch 1, Batch 200/1773, Loss: 8.5203, LR: 1.43e-05, Step time: 0.245s\n",
            "Epoch 1, Batch 300/1773, Loss: 7.8082, LR: 2.15e-05, Step time: 0.248s\n",
            "Epoch 1, Batch 400/1773, Loss: 7.1340, LR: 2.86e-05, Step time: 0.249s\n",
            "Epoch 1, Batch 500/1773, Loss: 6.5932, LR: 3.57e-05, Step time: 0.247s\n",
            "Epoch 1, Batch 600/1773, Loss: 6.1927, LR: 4.29e-05, Step time: 0.246s\n",
            "Epoch 1, Batch 700/1773, Loss: 5.7462, LR: 5.00e-05, Step time: 0.246s\n",
            "Epoch 1, Batch 800/1773, Loss: 5.6849, LR: 5.71e-05, Step time: 0.247s\n",
            "Epoch 1, Batch 900/1773, Loss: 5.5921, LR: 6.43e-05, Step time: 0.248s\n",
            "Epoch 1, Batch 1000/1773, Loss: 5.2248, LR: 7.14e-05, Step time: 0.247s\n",
            "Epoch 1, Batch 1100/1773, Loss: 5.2112, LR: 7.85e-05, Step time: 0.248s\n",
            "Epoch 1, Batch 1200/1773, Loss: 5.0968, LR: 8.57e-05, Step time: 0.247s\n",
            "Epoch 1, Batch 1300/1773, Loss: 4.8683, LR: 9.28e-05, Step time: 0.246s\n",
            "Epoch 1, Batch 1400/1773, Loss: 4.8380, LR: 9.99e-05, Step time: 0.246s\n",
            "\n",
            "==================================================\n",
            "EVALUATION AT STEP 1500\n",
            "==================================================\n",
            "Validation Loss: 4.5848\n",
            "Validation Perplexity: 97.98\n",
            "Token Accuracy: 42.91%\n",
            "BLEU Score: 5.30\n",
            "chrF Score: 17.35\n",
            "\n",
            "Sample Translations:\n",
            "----------------------------------------\n",
            "Reference 1: Mi sto sentendo molto instabile.\n",
            "Prediction 1: Sono sono molto molto molto..\n",
            "\n",
            "Reference 2: Io penso che la mia ragazza parta domani per la Scozia.\n",
            "Prediction 2: Io ho che la mia mia perando per. la mia..\n",
            "\n",
            "Reference 3: Di' loro che ho bisogno di un po' di soldi.\n",
            "Prediction 3: Mi'ho di ho bisogno di un po' di loro.\n",
            "\n",
            "âœ“ New best model saved! BLEU: 5.30\n",
            "==================================================\n",
            "\n",
            "Epoch 1, Batch 1500/1773, Loss: 4.6086, LR: 1.07e-04, Step time: 0.219s\n",
            "Epoch 1, Batch 1600/1773, Loss: 4.4354, LR: 1.14e-04, Step time: 0.224s\n",
            "Epoch 1, Batch 1700/1773, Loss: 4.2995, LR: 1.21e-04, Step time: 0.224s\n",
            "\n",
            "Epoch 1 completed. Average training loss: 6.0007\n",
            "Epoch 2, Batch 0/1773, Loss: 4.3373, LR: 1.27e-04, Step time: 0.250s\n",
            "Epoch 2, Batch 100/1773, Loss: 4.2848, LR: 1.34e-04, Step time: 0.246s\n",
            "Epoch 2, Batch 200/1773, Loss: 4.2855, LR: 1.41e-04, Step time: 0.247s\n",
            "Epoch 2, Batch 300/1773, Loss: 4.0822, LR: 1.48e-04, Step time: 0.246s\n",
            "Epoch 2, Batch 400/1773, Loss: 3.8880, LR: 1.55e-04, Step time: 0.247s\n",
            "Epoch 2, Batch 500/1773, Loss: 3.8703, LR: 1.62e-04, Step time: 0.245s\n",
            "Epoch 2, Batch 600/1773, Loss: 4.0721, LR: 1.69e-04, Step time: 0.247s\n",
            "Epoch 2, Batch 700/1773, Loss: 3.8198, LR: 1.76e-04, Step time: 0.247s\n",
            "Epoch 2, Batch 800/1773, Loss: 3.7925, LR: 1.84e-04, Step time: 0.246s\n",
            "Epoch 2, Batch 900/1773, Loss: 3.5785, LR: 1.91e-04, Step time: 0.246s\n",
            "Epoch 2, Batch 1000/1773, Loss: 3.5014, LR: 1.98e-04, Step time: 0.245s\n",
            "Epoch 2, Batch 1100/1773, Loss: 3.7749, LR: 2.05e-04, Step time: 0.246s\n",
            "Epoch 2, Batch 1200/1773, Loss: 3.6834, LR: 2.12e-04, Step time: 0.244s\n",
            "\n",
            "==================================================\n",
            "EVALUATION AT STEP 3000\n",
            "==================================================\n",
            "Validation Loss: 3.4320\n",
            "Validation Perplexity: 30.94\n",
            "Token Accuracy: 56.93%\n",
            "BLEU Score: 19.28\n",
            "chrF Score: 37.56\n",
            "\n",
            "Sample Translations:\n",
            "----------------------------------------\n",
            "Reference 1: Mi sto sentendo molto instabile.\n",
            "Prediction 1: Io sto sentendo molto gentileesso.\n",
            "\n",
            "Reference 2: Io penso che la mia ragazza parta domani per la Scozia.\n",
            "Prediction 2: Io penso che tu mia famiglia sarÃ erÃ  domani. essere miaina.\n",
            "\n",
            "Reference 3: Di' loro che ho bisogno di un po' di soldi.\n",
            "Prediction 3: Mi' loro che ne bisogno di un po' di soldi.\n",
            "\n",
            "âœ“ New best model saved! BLEU: 19.28\n",
            "==================================================\n",
            "\n",
            "Epoch 2, Batch 1300/1773, Loss: 3.3712, LR: 2.19e-04, Step time: 0.222s\n",
            "Epoch 2, Batch 1400/1773, Loss: 3.3395, LR: 2.26e-04, Step time: 0.224s\n",
            "Epoch 2, Batch 1500/1773, Loss: 3.0679, LR: 2.33e-04, Step time: 0.225s\n",
            "Epoch 2, Batch 1600/1773, Loss: 3.2914, LR: 2.41e-04, Step time: 0.223s\n",
            "Epoch 2, Batch 1700/1773, Loss: 3.1036, LR: 2.48e-04, Step time: 0.222s\n",
            "\n",
            "Epoch 2 completed. Average training loss: 3.6944\n",
            "Epoch 3, Batch 0/1773, Loss: 3.2447, LR: 2.53e-04, Step time: 0.248s\n",
            "Epoch 3, Batch 100/1773, Loss: 2.8936, LR: 2.60e-04, Step time: 0.245s\n",
            "Epoch 3, Batch 200/1773, Loss: 2.8730, LR: 2.67e-04, Step time: 0.245s\n",
            "Epoch 3, Batch 300/1773, Loss: 2.9815, LR: 2.74e-04, Step time: 0.245s\n",
            "Epoch 3, Batch 400/1773, Loss: 2.7688, LR: 2.81e-04, Step time: 0.248s\n",
            "Epoch 3, Batch 500/1773, Loss: 3.0089, LR: 2.89e-04, Step time: 0.245s\n",
            "Epoch 3, Batch 600/1773, Loss: 2.7201, LR: 2.96e-04, Step time: 0.247s\n",
            "Epoch 3, Batch 700/1773, Loss: 2.7145, LR: 3.03e-04, Step time: 0.247s\n",
            "Epoch 3, Batch 800/1773, Loss: 2.8056, LR: 3.10e-04, Step time: 0.246s\n",
            "Epoch 3, Batch 900/1773, Loss: 2.6242, LR: 3.17e-04, Step time: 0.246s\n",
            "\n",
            "==================================================\n",
            "EVALUATION AT STEP 4500\n",
            "==================================================\n",
            "Validation Loss: 2.6438\n",
            "Validation Perplexity: 14.07\n",
            "Token Accuracy: 70.18%\n",
            "BLEU Score: 40.04\n",
            "chrF Score: 60.53\n",
            "\n",
            "Sample Translations:\n",
            "----------------------------------------\n",
            "Reference 1: Mi sto sentendo molto instabile.\n",
            "Prediction 1: Io sto sentendo molto maletto.\n",
            "\n",
            "Reference 2: Io penso che la mia ragazza parta domani per la Scozia.\n",
            "Prediction 2: Penso penso che la mia morosa miga per. domani miazia.\n",
            "\n",
            "Reference 3: Di' loro che ho bisogno di un po' di soldi.\n",
            "Prediction 3: Dite' loro che ho bisogno di soldi po' di denaro.\n",
            "\n",
            "âœ“ New best model saved! BLEU: 40.04\n",
            "==================================================\n",
            "\n",
            "Epoch 3, Batch 1000/1773, Loss: 2.5856, LR: 3.24e-04, Step time: 0.222s\n",
            "Epoch 3, Batch 1100/1773, Loss: 2.5624, LR: 3.31e-04, Step time: 0.225s\n",
            "Epoch 3, Batch 1200/1773, Loss: 2.5460, LR: 3.39e-04, Step time: 0.225s\n",
            "Epoch 3, Batch 1300/1773, Loss: 2.5385, LR: 3.46e-04, Step time: 0.224s\n",
            "Epoch 3, Batch 1400/1773, Loss: 2.5106, LR: 3.53e-04, Step time: 0.223s\n",
            "Epoch 3, Batch 1500/1773, Loss: 2.2989, LR: 3.60e-04, Step time: 0.223s\n",
            "Epoch 3, Batch 1600/1773, Loss: 2.3895, LR: 3.67e-04, Step time: 0.224s\n",
            "Epoch 3, Batch 1700/1773, Loss: 2.4199, LR: 3.74e-04, Step time: 0.222s\n",
            "\n",
            "Epoch 3 completed. Average training loss: 2.7248\n",
            "Epoch 4, Batch 0/1773, Loss: 2.6926, LR: 3.79e-04, Step time: 0.250s\n",
            "Epoch 4, Batch 100/1773, Loss: 2.4274, LR: 3.87e-04, Step time: 0.245s\n",
            "Epoch 4, Batch 200/1773, Loss: 2.2830, LR: 3.94e-04, Step time: 0.246s\n",
            "Epoch 4, Batch 300/1773, Loss: 2.3737, LR: 4.01e-04, Step time: 0.246s\n",
            "Epoch 4, Batch 400/1773, Loss: 2.3689, LR: 4.08e-04, Step time: 0.246s\n",
            "Epoch 4, Batch 500/1773, Loss: 2.5719, LR: 4.15e-04, Step time: 0.246s\n",
            "Epoch 4, Batch 600/1773, Loss: 2.5344, LR: 4.22e-04, Step time: 0.247s\n",
            "\n",
            "==================================================\n",
            "EVALUATION AT STEP 6000\n",
            "==================================================\n",
            "Validation Loss: 2.3695\n",
            "Validation Perplexity: 10.69\n",
            "Token Accuracy: 74.72%\n",
            "BLEU Score: 46.78\n",
            "chrF Score: 66.73\n",
            "\n",
            "Sample Translations:\n",
            "----------------------------------------\n",
            "Reference 1: Mi sto sentendo molto instabile.\n",
            "Prediction 1: Mi sto sentendo molto.ol.\n",
            "\n",
            "Reference 2: Io penso che la mia ragazza parta domani per la Scozia.\n",
            "Prediction 2: Penso penso che la mia ragazza parr per. la Scozia.\n",
            "\n",
            "Reference 3: Di' loro che ho bisogno di un po' di soldi.\n",
            "Prediction 3: Dica' loro che ho bisogno di un po' di denaro.\n",
            "\n",
            "âœ“ New best model saved! BLEU: 46.78\n",
            "==================================================\n",
            "\n",
            "Epoch 4, Batch 700/1773, Loss: 2.1672, LR: 4.29e-04, Step time: 0.222s\n",
            "Epoch 4, Batch 800/1773, Loss: 2.2369, LR: 4.36e-04, Step time: 0.223s\n",
            "Epoch 4, Batch 900/1773, Loss: 2.2099, LR: 4.44e-04, Step time: 0.226s\n",
            "Epoch 4, Batch 1000/1773, Loss: 2.2401, LR: 4.51e-04, Step time: 0.224s\n",
            "Epoch 4, Batch 1100/1773, Loss: 2.4715, LR: 4.58e-04, Step time: 0.223s\n",
            "Epoch 4, Batch 1200/1773, Loss: 2.2505, LR: 4.65e-04, Step time: 0.224s\n",
            "Epoch 4, Batch 1300/1773, Loss: 2.1563, LR: 4.72e-04, Step time: 0.224s\n",
            "Epoch 4, Batch 1400/1773, Loss: 2.1548, LR: 4.79e-04, Step time: 0.222s\n",
            "Epoch 4, Batch 1500/1773, Loss: 2.1359, LR: 4.86e-04, Step time: 0.223s\n",
            "Epoch 4, Batch 1600/1773, Loss: 2.3212, LR: 4.94e-04, Step time: 0.224s\n",
            "Epoch 4, Batch 1700/1773, Loss: 2.3107, LR: 5.01e-04, Step time: 0.226s\n",
            "\n",
            "Epoch 4 completed. Average training loss: 2.3325\n",
            "Epoch 5, Batch 0/1773, Loss: 2.4655, LR: 5.06e-04, Step time: 0.250s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the trained model\n",
        "def test_model(model, test_sentences, tokenizer, device, max_len=50):\n",
        "    \"\"\"Test the model with custom sentences\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    pad_id = tokenizer.piece_to_id('<pad>')\n",
        "    sos_id = tokenizer.piece_to_id('<s>')\n",
        "    eos_id = tokenizer.piece_to_id('</s>')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence in test_sentences:\n",
        "            # Encode source\n",
        "            src_ids = tokenizer.encode(sentence, out_type=int)\n",
        "            src_ids = src_ids[:max_len-2]  # Leave room for padding\n",
        "            src_ids += [pad_id] * (max_len - len(src_ids))\n",
        "            src = torch.tensor([src_ids], dtype=torch.long).to(device)\n",
        "\n",
        "            # Generate translation\n",
        "            tgt_ids = [sos_id]\n",
        "            for _ in range(max_len - 1):\n",
        "                # Prepare decoder input\n",
        "                tgt_input = tgt_ids + [pad_id] * (max_len - len(tgt_ids))\n",
        "                tgt_tensor = torch.tensor([tgt_input], dtype=torch.long).to(device)\n",
        "\n",
        "                # Create masks\n",
        "                tgt_mask = generate_causal_mask(len(tgt_ids), device)\n",
        "                memory_key_padding_mask = (src == pad_id)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = model(src, tgt_tensor[:, :len(tgt_ids)],\n",
        "                             tgt_mask=tgt_mask,\n",
        "                             memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "                # Get next token\n",
        "                next_token = logits[0, -1].argmax().item()\n",
        "                tgt_ids.append(next_token)\n",
        "\n",
        "                # Stop at EOS\n",
        "                if next_token == eos_id:\n",
        "                    break\n",
        "\n",
        "            # Decode translation\n",
        "            translation_ids = [tid for tid in tgt_ids[1:] if tid not in {pad_id, eos_id}]\n",
        "            translation = tokenizer.decode(translation_ids, out_type=str)\n",
        "\n",
        "            results.append({\n",
        "                'source': sentence,\n",
        "                'translation': translation\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test with custom sentences\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I love programming.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"Can you help me with this problem?\",\n",
        "    \"Thank you very much.\"\n",
        "]\n",
        "\n",
        "translations = test_model(model, test_sentences, tokenizer, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRANSLATION EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "for result in translations:\n",
        "    print(f\"English: {result['source']}\")\n",
        "    print(f\"Italian: {result['translation']}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "wsYbFOmAbP0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}